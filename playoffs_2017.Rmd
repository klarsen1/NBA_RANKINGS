---
  output:
  md_document:
  variant: markdown_github
---
  
# And the Winner of the 2017 NBA Playoffs is... The Golden State Warriors!
  
As we all know, the 2017 NBA playoffs kicked off last weekend. But you didn't know that I've simulated the outcome by running the qualifying teams through playoff tree using the [Elastic NBA Ratings](https://github.com/klarsen1/NBA_RANKINGS).

I know you've been waiting for this for weeks (sarcasm intended) and so let's get right to the key results:
* First, a not-so-controversial preediction: The Golden State Warriors won the title in every single simulation, barely losing any games along the way. 
* More interestingly, however, is that the model picks the Boston Celtics as Golden State's opponent in the finals.
* Also, the model favors the Spurs over Houston to advance to the Western Conference Finals.

Obviously, this is just a model, and many things can happen. The style of play is different in the playoffs and injuries can strike at any time. But, barring any serious injuries, I feel confident that the Golden State will take the title this year. I feel less confident, however, that Boston will indeed beat the Cavs in a playoff series, and I also strongly doubt that San Antonio beat Houston -- despite having the better record. 

# About the Simulations

* No data from the 2017 playoffs were used to make the predictions. Only regular season data were used.
* Simulated each game in the playoff tree.
* For each game, the minutes played by each player were varied using a normal distribution where the mean and standard deviation are based on historical data.
* The playoffs were simulated 100 times. I tried more simulations as well but it didn't really alter the overall story.
* All injury information is current.
* The results described in the following section reflect the most commonly occurring outcomes.

# Should We Trust the Model?

First of all, we should never completely trust any statistical model. A healthy dose of skepticism and commmon sense should always be mixed with model predictions. Hence, model accuracy in itself is not a sufficient condition for a model to be useful, but it's an important factor.

When I first did a [backtest](https://github.com/klarsen1/NBA_RANKINGS) of the Elastic Rankings, the accuracy was around 66%. For the first half of the 2016-2017 season, the one-week-ahead accuracy was 63%. This was computed by comparing the predictions stored every Sunday to the games during the following week -- from the beginning of December to the end of February. Part of the decline in accuracy is due to the fact that the beginning of the season is harder to predict, part of of it is due to reasons that I have not dug into. 

Having said that, I decided that 63% is sufficient for me to go ahead and predict the playoffs while keeping a healthy dose of skepticism when it comes to the model's bullishness on the Celtics and Spurs.

# Breaking Down the Predictions

Let's say, for now, that we trust the model. The next questions are then: why does model favor the Warriors in every simulation? What's the big deal about Boston and how can San Antonio possibly beat the Rockets in a seven game series?

The answer these questions, we can decompose the playoff predictions across all simulations. 

## Decomposing Scores
As described in the [readme file](https://github.com/klarsen1/NBA_RANKINGS), we can decompose the predictions from the Elastic model into three parts:
 
* Roster -- archetype allocation deficits/surpluses. These are the variables labeled "share_minutes_cluster_XX" described above. This group of variables reflects the quality of the roster.
* Performance -- e.g., win percentages, previous match-ups.
* Circumstances -- e.g., travel, rest, home-court advantage
 
The decomposition essentially takes advantage of the fact that the model is additive in the log-odds scale to break down the prediction contributions. The code below plots the playoff predictions for all 100 simulations:
 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(knitr)
library(ggplot2)

f <-
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/modeldetails/2017_playoff_decomp.CSV"
 
center <- function(x){return(x-median(x))}
read.csv(f, stringsAsFactors = FALSE) %>%
  select(selected_team, roster, circumstances, performance) %>%
  group_by(selected_team) %>%
  #inner_join(qualifiers, by="selected_team") %>%
  summarise_each(funs(mean)) %>% ## get averages across games by team
  ungroup() %>%
  mutate_each(funs(center), which(sapply(., is.numeric))) %>% ## standardize across teams
  gather(modelpart, value, roster:performance) %>% ## transpose
  rename(team=selected_team) %>%
  ggplot(aes(team, value)) + geom_bar(aes(fill=modelpart), stat="identity") + coord_flip() +
  xlab("") + ylab("") + theme(legend.title = element_blank())
 
```

## So What Does this Chart Mean?

The bars show the contribution from each part of the model. Longer, more positive bars means that the team is stronger in that category compared to their playoff competitors, and vice versa for negative bars. Some interesting observations from this chart:

* The red bars show that circumstances -- e.g., traveling -- does not matter much in the playoffs.
* Golden state gets the highest score both in terms of performance (weighted winning percentage) and relative quality of the roster (surplus of stronger player archetypes). See the readme file for more details on this metrics. 
* The model impressed by San Antonio's ability to beat teams, but not impressed by its roster.
* The model favors Boston over Cleveland due to Cleveland's recent poor performance (illustrated by the green bars). Cleveland does well in terms of the quality and diversity of the roster, but not in terms of performance. But we all know that this can change in the playoffs.
* It seems like the model is picking home-court advantage and winning history over roster quality in the San Antonio match-up.

## Last Words

The only thing I know is that all models are wrong. Having said that, as stated in the intro, I feel confident that Golden State will reclaim the trophy this year -- barring any serious injuries. It seems like a safe prediction and the model seems to agree. 

As mentioned above, we always need to skeptical of models. Often, we must intervene because we know something the model doesn't know. For example, despite what the model says, I think (and hope) we'll see a match-up against Cleveland -- the recent performance we've seen from Cleveland is not indicative of what they can do in the playoffs. 




  