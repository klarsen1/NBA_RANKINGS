---
title: "Elastic NBA Team Rankings"
output: html_document
---
 

# Prelude -- What Should We Expect From Predictive Models?
 
After the 2016 presidential election, most people would probably agree that the world does not need another statistical prediction model. The election models were wrong (although Nate Silver got pretty close) and that likely added to the element of shock and surprise -- on both sides of the political spectrum.
 
So does this mean that we should we turn our backs on forecasting models? No, we just need to revise our expectations. George Box once reminded us that statistical models are, at best, useful *approximations* of the real world. With the recent hype around data science and "money balling" this point is often overlooked.

What *should* we then expect from a statistical model? A statistical model should help us process the many moving parts that often affect a given outcome -- such as an election -- by providing a single prediction for the future. It does this by combining the multitude of inputs, assumptions, trends and correlations with a simplified representation of how the world works. The human mind cannot do this and that is what makes models so valuable. However, if the inputs are bad even a good model is going to be wrong. Moreover, no model predicts the unthinkable it unless unthinkable assumptions are made.
 
# Forecasting and Basketball 

In many ways, basketball is a perfect case study for forecasting. The outcome of future games are affected by many different factors; the team's current performance, momentum, the strength of the roster compared to its opponents, as well as the travel schedule. If a team looks great on paper and it's winning games it'll likely do well in the future. But injuries, coaching changes, trades etc. can curtail success very quickly. Thus, any model-based prediction can only be viewed as a benchmark for future success -- i.e., setting expectations is key

This post discusses a new and data driven approach to predicting the outcome of NBA games. The model uses techniques frequently used across various industries to predict bankruptcy, fraud or customer buying behavior. While it's certain that the model is wrong -- all models are -- I'm hoping that it will also be useful.

# High Level Summary
 
The team rankings produced by this model mainly agree with other prediction models (such as FiveThirtyEight) -- at least when it comes to identifying the strong teams and the weak teams (the "tail teams"). There are some interesting differences, however, which we can analyze using a model-decomposition approach. Examples of how to do this will be covered later in this document.

Back testing for the 2015-2016 season showed promising results, although I have not done any historical benchmark testing against other models. Time will be the judge.

All rankings and scores can be found in [this github repo](https://github.com/klarsen1/NBA_RANKINGS). The easiest way to extract the data is to directly read the [raw files](https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/modeldetails). This is demonstrated in the examples below.
 
There are two main file of interest:
 
* game_level_predictions_2016-MM-DD.csv -- Game-level predictions for each future game.
* rankings_2016-MM-DD.csv -- team rankings and predicted win rates.

In addition, the [modeldetails directory](https://github.com/klarsen1/NBA_RANKINGS/tree/master/modeldetails) has detailed information on the underlying mechanics of the model.
 
# How the Model Works
 
The Elastic NBA Team Rankings is a dynamic ranking algorithm that is purely based on statistical modeling techniques commonly used across most industries. No qualitative data or judgment is used to decide the ranks or the importance of different variables; the only human judgment applied is the underlying framework behind the algorithm.
 
At a high level, the model depends on three overall factors:
 
* Previous performance.
* How the team looks on paper. This is measured by the roster composition of "player archetypes."
* Circumstances -- e.g., traveling, rest days, home court advantage.
 
## More Detailed Description
The model is based on three-step procedure:
 
1. Create 25 data-driven *archetypes* using [k-means clustering ](https://en.wikipedia.org/wiki/K-means_clustering) based on game-level box score statistics from games prior to the 2016-2017 season. The goal of the clustering algorithm is to maximize similarity of players (in terms of offensive and defensive stats) *within* clusters, while minimizing differences *between* clusters. For games during the 2016-2017 season, players are mapped to a given cluster based on their recent performance. 
2. The winner of a given game is predicted based on team archetypes, home team advantage, rest days, miles traveled, previous match ups between the two teams (during that season), as well as recent win percentages.
3. Teams are ranked based on the predicted win rate for the season. Hence the ranking is schedule-dependent.
 
Why is the model called "Elastic NBA Rankings?" There are two reasons for this: first, the model automatically adapts as the season progresses. Second, the regularization technique used to fit the logistic regression model is a special case of the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization).
 
### Some Notes on the Model Used in Step 2
The model used to predict the winner of a given game is a statistical model that is estimated based on the most recent three seasons. Hence, the relative importance (weights) of the various drivers -- for example, the importance of roster features versus win percentages -- are purely based on the relationship detected from the data. For the stats-minded readers, the model is a logistic regression with an L1 penalty (lasso) to reduce the chance of over fitting (this worked best in back-testing).
 
The model is re-estimated every single day and contains the following variables:
 
*  Roster composition -- surplus/deficit of minutes allocated to the different archetypes. For example, if a team's lineup has more players on the court of archetype 1 than its opponent, it'll have a surplus of minutes allocated to that archetype, and vice versa.
* Trailing 90 day winning percentages -- the model assigns less importance to win-streaks early in the season. Moreover, the model assigns higher importance to wins where the opponent has a high [CARM-ELO](http://fivethirtyeight.com/features/how-our-2015-16-nba-predictions-work/) score.
* Previous match-up outcomes -- for example, let's say Golden State is playing the Clippers and the two teams have played each other earlier in the season. This variable captures the outcomes of those two games. A team is more likely to win if it beat its opponent in the past.
* Distances traveled prior to the game -- traveling the day before games usually translates into weaker performances, holding everything else constant.
* Rest days prior to games -- more rest is beneficial during the long NBA season.
* Home team advantage.
 
### More Details on the Archetype Surplus/Deficit Variables
For a given game, the model does the following calculations:

#####Team 1:
\\( X_1 = \\)  % of minutes allocated to cluster 1, \\( X_{2} = \\)  % of minutes allocated to cluster 2, etc.
 
#####Team 2:
\\( Z_1 = \\)  % of minutes allocated to cluster 1, \\( Z_{2} = \\)  % of minutes allocated to cluster 2, etc.
 
From these variables we construct the “delta variables” given by
 
\\( D_{1} = X_{1} – Z_{1}, D_{2} = X_{2} – Z_{2} \\), etc.
 
These variables are then directly entered into the logistic regression model (labeled as share_minutes_cluster_XX). The regression model then estimates the importance of each archetype. Hence, for team 1's roster to be considered strong, compared team 2, it must have a surplus of minutes allocated to archetypes with large and positive coefficients, and vice versa for archetypes with negative coefficients.
 
 
### How Are Players Assigned to Archetypes?
 
The outcome of the k-means clustering routine used create the archetypes is a set of *centroids*. Players are assigned to archetypes by matching their offensive and defensive box score statistics to closest centroids using a simple Euclidean distance. A decay function was applied such that more recent games receive a larger weight. In addition, games played in the previous season are discounted by a factor of 4.
 
### Predicting Allocation of Minutes for Future Games
 
In order to calculate the deficit and surplus variables referenced above, it's necessary to predict how many minutes each player will play. Currently, a 90-day trailing average is used (excluding the off-season). Games played during the prior season are discounted by a factor of 4.
 
### Calling the Games
The current implementation uses the estimated probabilities from the regularized logistic regression model to pick the winner of a given game. If the estimated probability of a given team winning exceeds 50%, then that team is declared the winner.
 
I've also been playing around with a simulation approach where each games is "played"" 1000 times, varying the distribution of minutes across archetypes in each iteration. For the season rankings, this did not alter the overall conclusion, although it did provide a better measure of prediction uncertainties.
 
For simulation playoffs I have been using the simulation approach. This will be covered in another post.
 
# Model Rankings for the 2016-2017 Season
 
All model rankings and results are stored in [this github repo](https://github.com/klarsen1/NBA_RANKINGS). The code below shows how to extract the current rankings and compare to the [FiveThirtyEight win/loss predictions](http://projects.fivethirtyeight.com/2017-nba-predictions/). The folks at FiveThirtyEight do amazing work and so this seems like a good sanity check.
 
Note that the rankings file stored github repo has three key columns:
 
* ytd_win_rate -- this is the year-to-date win rate for the season.
* pred_win_rate -- this is the predicted win rate for future games.
* pred_season_win_rate -- this combines the predicted games with the games that have been played. This is the statistic I'm using to rank teams in the example below.
 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(knitr)
 
f1 <-
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/rawdata/FiveThirtyEight_2016-11-20.csv"
f2 <-
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/rankings/rankings_2016-11-20.csv"
 
ft8_rankings <- read.csv(f1) %>% rename(team=selected_team)
 
all_rankings <- read.csv(f2) %>%
  inner_join(ft8_rankings, by="team") %>%
  mutate(elastic_ranking=min_rank(-pred_season_win_rate),
         FiveThirtyEight=min_rank(-pred_win_rate_538)) %>%
  select(team, conference, division, elastic_ranking, FiveThirtyEight) %>%
  arrange(elastic_ranking)
 
kable(all_rankings)
```
 
The table shows that the elastic rankings generally agree with FiveThirtyEight -- at least when it comes to the "tail teams." For example, all rankings agree that Golden State, Cleveland and the Clippers will have strong seasons, while Philadelphia and New Orleans will struggle to win games.
 
But what about Atlanta? The elastic model ranks Atlanta fourth in terms of overall win percentage, while FiveThirtyEight ranks Atlanta at number 11. To understand why the elastic model is doing this, we can decompose the predictions into three parts:
 
* Roster -- archetype allocation deficits/surpluses. These are the variables labelled "share_minutes_cluster_XX" described above. This group reflects the quality of the roster.
* Performance -- e.g., win percentages, previous match ups.
* Circumstances -- e.g., travel, rest, home court advantage
 
For the stats-oriented readers, here's how this works: the underlying predictive variables were multiplied by their respective coefficients, and then aggregated to get the group contributions to the predicted log-odds. The CSV file called score_decomp_2016_MM_DD contains this information. The code below shows how to use this file:
 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(knitr)
library(ggplot2)
 
f <-
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/modeldetails/score_decomp_2016-11-20.csv"
 
center <- function(x){return(x-median(x))}
read.csv(f, stringsAsFactors = FALSE) %>%
  select(selected_team, roster, circumstances, performance) %>%
  group_by(selected_team) %>%
  summarise_each(funs(mean)) %>% ## get averages across games by team
  ungroup() %>%
  mutate_each(funs(center), which(sapply(., is.numeric))) %>% ## standardize across teams
  gather(modelpart, value, roster:performance) %>% ## transpose
  rename(team=selected_team) %>%
  ggplot(aes(team, value)) + geom_bar(aes(fill=modelpart), stat="identity") + coord_flip() +
  xlab("") + ylab("") + theme(legend.title = element_blank())
 
```
 
Here's how to interpret the chart: the bars show the contribution from each part of the model. As expected, circumstances do not affect the overall prediction for the entire season as most teams have similarly taxing schedules. However, contributions from performance (weighted winning percentages) and roster (surplus of important archetypes) vary considerably.
 
First, let's take a look at Atlanta: the model thinks that Atlanta’s roster is average (relative to its opponents). However, Atlanta has been winning a high percentage of their games lately -- they even beat Cleveland -- and thus we get a significant positive contribution from this (recall that the wins are weighted by the opponents [CARM-ELO](http://projects.fivethirtyeight.com/2017-nba-predictions/) rating). 
 
Next, let's look at Cleveland and Golden State. The model ranks these two teams at the top, both in terms of rosters and performance. In fact, my playoff simulations have these two teams meeting again in the finals and going to seven games (more on that in a later post).
 
Last, but not least, let's take a look at San Antonio and the Timberwolves -- two teams that are viewed very differently by the model. According to the model, San Antonio has been over performing. The model does not like how the roster looks on paper, yet performance has been strong so far. This could be due to strong coaching, "corporate knowledge" (as Gregg Popovich calls it) and team chemistry -- factors that the roster component of the model does not capture. Minnesota, on the other hand, is under performing according to the model; the roster is rated highly compared to its opponents, but the team is not performing well. This could be due to inexperience.
 
# Backtesting
 
The model was used to predict all games from 2015-11-17 to the end the 2015-2016 season, using only information available as of 2015-11-16. Most teams played 71 games during this period.
 
The game level accuracy was 67% -- i.e., the model predicted the correct winner in 67% of cases.
 
The [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) was 0.736.
 
The predicted team rankings were also in line with the actual team rankings (correlation is 75%), except for some interesting misses like the Chicago Bulls (actual ranking #14, predicted #4). Note that this table only covers the games played during the 2015-2016 season after 2015-11-16.
 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(dplyr)
library(knitr)
 
f <-
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/rankings/ranking_validation_2015.csv"
 
rankings <- read.csv(f) %>% select(team, rank_actual, rank_pred) %>% arrange(-rank_pred)
 
kable(rankings)
 
```
 
# Future Development
 
### Dealing With Injuries and Trades
As mentioned previously, the model depends on rosters *and* previous performance. If a team executes a major mid-season trade, the roster component will react immediately, while the performance component will be slower to react. There are a number of ways around this, but currently no special treatment is being applied at this point.
 
### Player Interaction
The model currently does not capture any interaction between the archetypes. I have tested basic interaction terms, but that did not help much. More work is needed here.
 
### Schedule-independent Team Rankings
Currently, the model ranks teams by predicting win rates. This means that, holding everything else constant, the rankings implicitly favor teams that play in weaker divisions. A future development could be to have two rankings: one that predicts the win rate given the current schedule (this is what the model is currently doing), and one that normalizes for schedule differences.


